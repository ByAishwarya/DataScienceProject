{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f9ab0aa",
   "metadata": {},
   "source": [
    "## Finally!ðŸ¥³"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3c7476",
   "metadata": {},
   "source": [
    "### Saving the Preprocessing Pipeline\n",
    "\n",
    "We have now invested quite some effort into building and fitting our preprocessing pipeline â€” handling missing values, scaling numeric features, and encoding categorical variables.\n",
    "\n",
    "To make sure we can reproduce exactly the same preprocessing steps later, even after closing this notebook or when working in a different script, we can **save the fitted pipeline** to disk.\n",
    "\n",
    "This allows us to:\n",
    "- Reuse the exact same transformations on new data (e.g. test data or data in production)\n",
    "- Ensure full reproducibility of our workflow\n",
    "- Experiment with different preprocessing strategies and compare them fairly\n",
    "- Keep preprocessing and modeling steps **consistent and versioned**\n",
    "\n",
    "In short, saving the pipeline means that our entire data preparation process becomes **reusable, consistent, and shareable** â€” an essential part of any professional machine learning workflow.\n",
    "\n",
    "(From Notes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7737bc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "# Even though you don't call sklearn directly to load the file, \n",
    "# joblib needs it in the background to reconstruct the objects.\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b752d01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the preprocessor and feature set list\n",
    "loaded_preprocessor = joblib.load(\"../models/standard_scaler.joblib\")\n",
    "with open('../models/feature_sets.json', 'r') as f:\n",
    "    feature_sets = json.load(f)\n",
    "\n",
    "# 2. Load raw data\n",
    "df_new = pd.read_csv(\"../data/dataset.csv\")\n",
    "\n",
    "# 3. RECREATE ENGINEERED FEATURES \n",
    "# Math features\n",
    "df_new['duration_min'] = df_new['duration_ms'] / 60000\n",
    "df_new['energy_x_danceability'] = df_new['energy'] * df_new['danceability']\n",
    "df_new['loudness_x_energy'] = df_new['loudness'] * df_new['energy']\n",
    "df_new['valence_x_danceability'] = df_new['valence'] * df_new['danceability']\n",
    "df_new['tempo_log'] = np.log1p(df_new['tempo'])\n",
    "\n",
    "# Logical/Categorical features (This fixes your KeyError)\n",
    "df_new['is_instrumental'] = df_new['instrumentalness'] > 0.5\n",
    "df_new['has_vocals'] = df_new['instrumentalness'] < 0.5\n",
    "df_new['is_speech_heavy'] = df_new['speechiness'] > 0.66\n",
    "\n",
    "# Duration Category (Example logic - ensure this matches your first notebook)\n",
    "df_new['duration_category'] = pd.cut(df_new['duration_min'], \n",
    "                                     bins=[0, 2, 4, 10, 100], \n",
    "                                     labels=['short', 'medium', 'long', 'very_long'])\n",
    "\n",
    "# 4. NOW SELECT THE FEATURES\n",
    "X_new = df_new[feature_sets['full']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c535829e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
