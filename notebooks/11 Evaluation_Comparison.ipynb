{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9202522c",
   "metadata": {},
   "source": [
    "# Model Performance Analysis Report\n",
    "\n",
    "## üìä Summary Table of Best Performers Per Dataset\n",
    "\n",
    "| Dataset | Best R¬≤ Model | Best Accuracy (¬±15%) Model | Best Speed (Train+Pred) | Worst Overall |\n",
    "|---------|---------------|----------------------------|-------------------------|---------------|\n",
    "| Full Attributes | Random Forest (0.4716) | Random Forest (78.21%) | Linear Regression (0.2345s) | Lasso Regression |\n",
    "| Top Attributes | Random Forest (0.3845) | Random Forest (73.27%) | Linear Regression (0.1033s) | Lasso Regression |\n",
    "| Reduced Attributes | Random Forest (0.3844) | Random Forest (73.12%) | Linear Regression (0.0731s) | Lasso Regression |\n",
    "\n",
    "## üìà Detailed Performance Comparison\n",
    "\n",
    "### Full Attributes Dataset (Most Features)\n",
    "| Model | MAE | RMSE | R¬≤ Score | Acc ¬±15% | Train Time | Pred Time |\n",
    "|-------|-----|------|----------|----------|------------|-----------|\n",
    "| **Random Forest** | **10.02** | **14.87** | **0.4716** | **78.21%** | 401.72s | 1.542s |\n",
    "| Decision Tree | 11.48 | 16.62 | 0.3396 | 72.99% | 2.61s | 0.016s |\n",
    "| Gradient Boosting | 11.91 | 16.69 | 0.3342 | 71.32% | 57.64s | 0.066s |\n",
    "| Linear Regression | 11.98 | 16.83 | 0.3231 | 71.03% | 0.22s | 0.014s |\n",
    "| Ridge Regression | 11.98 | 16.83 | 0.3231 | 71.03% | 0.093s | 0.010s |\n",
    "| Lasso Regression | 12.00 | 16.83 | 0.3227 | 71.04% | 0.350s | 0.003s |\n",
    "\n",
    "### Top Attributes Dataset (Feature Selected)\n",
    "| Model | MAE | RMSE | R¬≤ Score | Acc ¬±15% | Train Time | Pred Time |\n",
    "|-------|-----|------|----------|----------|------------|-----------|\n",
    "| **Random Forest** | **11.30** | **16.04** | **0.3845** | **73.27%** | 7.79s | 0.105s |\n",
    "| Gradient Boosting | 11.90 | 16.68 | 0.3348 | 71.40% | 39.43s | 0.067s |\n",
    "| Decision Tree | 11.59 | 16.72 | 0.3315 | 73.22% | 1.30s | 0.008s |\n",
    "| Linear Regression | 11.98 | 16.83 | 0.3225 | 71.03% | 0.064s | 0.040s |\n",
    "| Ridge Regression | 11.98 | 16.83 | 0.3225 | 71.03% | 0.067s | 0.005s |\n",
    "| Lasso Regression | 12.00 | 16.84 | 0.3222 | 71.11% | 0.186s | 0.006s |\n",
    "\n",
    "### Reduced Attributes Dataset (Fewest Features)\n",
    "| Model | MAE | RMSE | R¬≤ Score | Acc ¬±15% | Train Time | Pred Time |\n",
    "|-------|-----|------|----------|----------|------------|-----------|\n",
    "| **Random Forest** | **11.31** | **16.05** | **0.3844** | **73.12%** | 8.37s | 0.142s |\n",
    "| Gradient Boosting | 11.90 | 16.67 | 0.3352 | 71.39% | 39.19s | 0.066s |\n",
    "| Decision Tree | 11.59 | 16.69 | 0.3343 | 72.57% | 1.29s | 0.017s |\n",
    "| Linear Regression | 11.99 | 16.84 | 0.3219 | 71.09% | 0.066s | 0.007s |\n",
    "| Ridge Regression | 11.99 | 16.84 | 0.3219 | 71.09% | 0.066s | 0.006s |\n",
    "| Lasso Regression | 12.00 | 16.84 | 0.3220 | 71.13% | 0.123s | 0.007s |\n",
    "\n",
    "## üîç Key Findings & Observations\n",
    "\n",
    "### **Performance Insights:**\n",
    "1. **Random Forest consistently outperforms** all other models across all datasets in both R¬≤ score and Accuracy (¬±15%)\n",
    "2. **Full attributes dataset yields best overall performance** - Random Forest achieves R¬≤=0.4716 vs 0.3845 with reduced features\n",
    "3. **Linear models (Linear/Ridge/Lasso) show minimal differences** - nearly identical performance across all metrics\n",
    "4. **Feature reduction impacts tree-based models most** - Random Forest performance drops significantly from full to reduced features\n",
    "\n",
    "### **Efficiency Analysis:**\n",
    "1. **Training Time**: Random Forest is slowest (401s full, ~8s reduced), Linear Regression fastest (~0.07s)\n",
    "2. **Prediction Time**: All models are fast (<1.5s), with Linear/Ridge/Lasso being fastest\n",
    "3. **Speed-Accuracy Trade-off**: Random Forest offers best accuracy but slowest training; Linear models offer fastest training but lowest accuracy\n",
    "\n",
    "### **Model Behavior:**\n",
    "1. **Tree-based models** (Random Forest, Decision Tree, Gradient Boosting) benefit more from additional features\n",
    "2. **Linear models** show minimal improvement with additional features\n",
    "3. **Decision Tree** offers best balance of speed and performance among tree-based models\n",
    "4. **Gradient Boosting** shows consistent but not exceptional performance across datasets\n",
    "\n",
    "### **Metric Observations:**\n",
    "1. **R¬≤ Score**: Ranges from 0.322 to 0.472 (best)\n",
    "2. **Accuracy (¬±15%)**: Ranges from 71.03% to 78.21% (best)\n",
    "3. **RMSE**: Ranges from 14.87 to 16.84 (lower is better)\n",
    "4. **MAE**: Ranges from 10.02 to 12.00 (lower is better)\n",
    "\n",
    "## üí° Recommendations\n",
    "\n",
    "1. **For maximum accuracy**: Use **Random Forest with full attributes** (R¬≤=0.4716, Acc=78.21%)\n",
    "2. **For production with constraints**: Consider **Random Forest with top attributes** (good balance of speed and accuracy)\n",
    "3. **For rapid prototyping**: Use **Linear Regression** (fastest training, reasonable performance)\n",
    "4. **For interpretability**: Consider **Decision Tree** (good performance, fast, interpretable)\n",
    "\n",
    "## üìù Notes on Models & Metrics Used\n",
    "\n",
    "**Models Evaluated:**\n",
    "- Linear Regression (baseline)\n",
    "- Ridge Regression (L2 regularization)\n",
    "- Lasso Regression (L1 regularization)\n",
    "- Decision Tree (non-linear, interpretable)\n",
    "- Random Forest (ensemble, best performer)\n",
    "- Gradient Boosting (sequential ensemble)\n",
    "\n",
    "**Evaluation Metrics:**\n",
    "- **MAE (Mean Absolute Error)**: Average absolute difference between predictions and actual values\n",
    "- **RMSE (Root Mean Square Error)**: Square root of average squared differences (penalizes large errors)\n",
    "- **R¬≤ Score**: Proportion of variance explained (0-1, higher is better)\n",
    "- **Median AE**: Median absolute error (robust to outliers)\n",
    "- **Accuracy ¬±15%**: Percentage of predictions within 15% of actual values\n",
    "- **Max Error**: Worst-case prediction error\n",
    "- **Train/Pred Time**: Computational efficiency metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb8ff04",
   "metadata": {},
   "source": [
    "### Evaluation Metric Consideration\n",
    "For my evaluation I didnt consider Mean Absolute Percentage Error (MAPE), because it was giving me really really high values, like exponential to 18.\n",
    "\n",
    "This happened because because a lot of Target Values (Popularity) were close to 0, and since MAPE is calculated like this: |actual - predicted| / actual √ó 100\n",
    "\n",
    "In the denominator it's actual, in many cases it as 0.\n",
    "\n",
    "Hence MAPE was not suitable for my dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd2c831",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9248a66",
   "metadata": {},
   "source": [
    "## Training Note: Grid Search Limitation\n",
    "\n",
    "**Important Experimental Observation:**\n",
    "\n",
    "During this study, I attempted to perform **Grid Search hyperparameter optimization** but only for the **top attributes dataset** due to computational constraints.\n",
    "\n",
    "**Why Grid Search was limited to top attributes only:**\n",
    "1. **Computational Complexity**: Grid Search with full attributes would have been computationally infeasible\n",
    "2. **Parameter Interactions**: Parameters interact strongly with genre attributes, creating complex search spaces\n",
    "3. **Time Constraints**: For the top attributes dataset with grid search:\n",
    "   - 432 candidate parameter combinations\n",
    "   - 5-fold cross-validation for each\n",
    "   - **Total: 2,160 model fits**\n",
    "   - **Estimated training time: 4-5 days** on my current hardware\n",
    "\n",
    "**Technical Challenge:**\n",
    "- Even after adjusting hyperparameters, computational time remained prohibitive\n",
    "- My PC hardware couldn't support the extensive computation required\n",
    "- This highlights the exponential complexity growth with feature count\n",
    "\n",
    "**Future Improvement Strategy:**\n",
    "For future work, I plan to implement **Random Search with 20 iterations**  because:\n",
    "1. **Faster convergence**: Random search often finds good parameters faster\n",
    "2. **Better computational efficiency**: 20 iterations vs. 2,160 fits of grid search\n",
    "3. **Proven effectiveness**: Research shows random search can outperform grid search in high-dimensional spaces\n",
    "4. **Practical feasibility**: Can be completed within reasonable timeframes\n",
    "\n",
    "**Implication for Results:**\n",
    "The current model performances represent **baseline performances without extensive hyperparameter tuning**. The reported metrics could potentially be improved with proper optimization using more efficient search strategies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
