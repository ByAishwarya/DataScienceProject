{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Notebook 11: Model Evaluation & Comparison\n",
    "\n",
    "## Overview\n",
    "This notebook provides a **comprehensive comparison** of all three models developed in this project:\n",
    "\n",
    "```\n",
    "Model A (Notebook 08): Linear Regression\n",
    "‚îú‚îÄ Linear Regression (baseline)\n",
    "‚îú‚îÄ Ridge Regression (L2 regularization)\n",
    "‚îî‚îÄ Lasso Regression (L1 regularization)\n",
    "\n",
    "Model B (Notebook 09): Random Forest Regressor\n",
    "‚îú‚îÄ Hyperparameter tuning (24 configurations)\n",
    "‚îú‚îÄ Feature set comparison (Full/Reduced/Top)\n",
    "‚îî‚îÄ Non-linear ensemble approach\n",
    "\n",
    "Model C (Notebook 10): Stacking Regressor\n",
    "‚îú‚îÄ 3 base models (Linear, Ridge, RF)\n",
    "‚îú‚îÄ Meta-model (Ridge)\n",
    "‚îî‚îÄ Hyperparameter tuning (432 configurations)\n",
    "```\n",
    "\n",
    "## Evaluation Framework\n",
    "\n",
    "### Metrics Analyzed:\n",
    "1. **RMSE (Root Mean Squared Error)**: Primary metric, penalizes large errors\n",
    "2. **MAE (Mean Absolute Error)**: Interpretable average error\n",
    "3. **R¬≤ Score**: Proportion of variance explained (0-1)\n",
    "4. **Training Time**: Computational efficiency\n",
    "5. **Model Complexity**: Number of hyperparameters\n",
    "\n",
    "### Visualizations:\n",
    "- Performance comparison charts\n",
    "- Actual vs Predicted plots (all models)\n",
    "- Residual analysis\n",
    "- Error distribution histograms\n",
    "- Hyperparameter impact analysis\n",
    "- Feature importance comparison\n",
    "\n",
    "### Analysis Sections:\n",
    "1. **Performance Metrics Comparison**\n",
    "2. **Hyperparameter Analysis**\n",
    "3. **Prediction Quality Assessment**\n",
    "4. **Model Complexity vs Performance**\n",
    "5. **Best Model Recommendation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# LIBRARY IMPORTS\n",
    "# ================================================================================\n",
    "\n",
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning - Models\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, StackingRegressor\n",
    "\n",
    "# Machine Learning - Evaluation\n",
    "from sklearn.metrics import (\n",
    "    root_mean_squared_error,    # RMSE - primary metric\n",
    "    mean_absolute_error,        # MAE - interpretable error\n",
    "    r2_score,                   # R¬≤ - variance explained\n",
    "    mean_squared_error          # MSE - for calculations\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# Utilities\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"‚úì All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# VISUAL STYLING CONFIGURATION\n",
    "# ================================================================================\n",
    "# Consistent Spotify-inspired theme across all visualizations\n",
    "\n",
    "# Color palette\n",
    "BG = '#e1ece3'           # Light mint background\n",
    "PRIMARY = '#62d089'      # Spotify green\n",
    "EMPHASIS = '#457e59'     # Dark green for emphasis\n",
    "GRID = '#a8b2a8'         # Subtle gray grid\n",
    "\n",
    "# Color scheme for different models\n",
    "MODEL_COLORS = {\n",
    "    'Linear': '#3498db',      # Blue\n",
    "    'Ridge': '#2ecc71',       # Green  \n",
    "    'Lasso': '#1abc9c',       # Teal\n",
    "    'Random Forest': '#e74c3c',  # Red\n",
    "    'Stacking': '#9b59b6'     # Purple\n",
    "}\n",
    "\n",
    "# Apply matplotlib settings\n",
    "plt.rcParams.update({\n",
    "    'figure.facecolor': BG,\n",
    "    'axes.facecolor': BG,\n",
    "    'axes.edgecolor': BG,\n",
    "    'axes.labelcolor': '#2b2b2b',\n",
    "    'xtick.color': '#2b2b2b',\n",
    "    'ytick.color': '#2b2b2b',\n",
    "    'grid.color': GRID,\n",
    "    'grid.alpha': 0.4,\n",
    "    'axes.grid': True,\n",
    "    'font.size': 11,\n",
    "    'figure.titlesize': 14,\n",
    "    'axes.titlesize': 12,\n",
    "    'axes.labelsize': 11\n",
    "})\n",
    "\n",
    "# Seaborn style\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"‚úì Visual styling configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÇ Load Data & Train Models\n",
    "\n",
    "To ensure fair comparison, we'll:\n",
    "1. Use the **same test set** for all models\n",
    "2. Use **Top 12 features** (best performer across all models)\n",
    "3. Train each model with **optimal hyperparameters** from previous notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# LOAD PREPROCESSED DATA\n",
    "# ================================================================================\n",
    "\n",
    "print(\"Loading data...\")\n",
    "\n",
    "# Load target variable\n",
    "y_train = pd.read_csv('../data/y_train.csv').values.ravel()\n",
    "y_test = pd.read_csv('../data/y_test.csv').values.ravel()\n",
    "\n",
    "# Load Top feature set (best performer)\n",
    "X_train = pd.read_csv('../data/X_train_top.csv')\n",
    "X_test = pd.read_csv('../data/X_test_top.csv')\n",
    "\n",
    "# Display data info\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Training samples:   {len(X_train):,}\")\n",
    "print(f\"Testing samples:    {len(X_test):,}\")\n",
    "print(f\"Features:           {X_train.shape[1]}\")\n",
    "print(f\"Feature set:        Top 12 features\")\n",
    "print(f\"Target variable:    Popularity (0-100)\")\n",
    "print(f\"Target mean:        {y_train.mean():.2f}\")\n",
    "print(f\"Target std:         {y_train.std():.2f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Store feature names for later use\n",
    "feature_names = X_train.columns.tolist()\n",
    "print(f\"\\nFeatures: {', '.join(feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Train All Models with Optimal Hyperparameters\n",
    "\n",
    "### Model A: Linear Models\n",
    "**From Notebook 08 - Best Configuration:**\n",
    "- **Ridge Regression**: alpha = 10 (best linear model)\n",
    "- **Lasso Regression**: alpha = 0.1\n",
    "- **Linear Regression**: No hyperparameters (baseline)\n",
    "\n",
    "### Model B: Random Forest\n",
    "**From Notebook 09 - Best Configuration (Top Features):**\n",
    "- **n_estimators**: 200 trees\n",
    "- **max_depth**: None (unlimited)\n",
    "- **min_samples_split**: 2\n",
    "- **min_samples_leaf**: 1\n",
    "- **max_features**: 'sqrt'\n",
    "\n",
    "### Model C: Stacking Regressor\n",
    "**From Notebook 10 - Best Configuration:**\n",
    "- **Base Models**: Linear, Ridge (alpha=10), RF (200 trees, depth=None, split=2)\n",
    "- **Meta-Model**: Ridge (alpha=1.0)\n",
    "- **CV Strategy**: 5-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# TRAIN ALL MODELS WITH OPTIMAL HYPERPARAMETERS\n",
    "# ================================================================================\n",
    "\n",
    "print(\"Training all models with optimal hyperparameters...\\n\")\n",
    "\n",
    "# Dictionary to store all models and their metadata\n",
    "models = {}\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# MODEL A: LINEAR MODELS (Notebook 08)\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "print(\"[1/5] Training Linear Regression...\")\n",
    "start = time.time()\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "models['Linear'] = {\n",
    "    'model': linear_model,\n",
    "    'category': 'Linear Models',\n",
    "    'notebook': 'Notebook 08',\n",
    "    'hyperparameters': 'None (baseline)',\n",
    "    'training_time': time.time() - start\n",
    "}\n",
    "\n",
    "print(\"[2/5] Training Ridge Regression (alpha=10)...\")\n",
    "start = time.time()\n",
    "ridge_model = Ridge(alpha=10, random_state=42)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "models['Ridge'] = {\n",
    "    'model': ridge_model,\n",
    "    'category': 'Linear Models',\n",
    "    'notebook': 'Notebook 08',\n",
    "    'hyperparameters': 'alpha=10',\n",
    "    'training_time': time.time() - start\n",
    "}\n",
    "\n",
    "print(\"[3/5] Training Lasso Regression (alpha=0.1)...\")\n",
    "start = time.time()\n",
    "lasso_model = Lasso(alpha=0.1, random_state=42, max_iter=10000)\n",
    "lasso_model.fit(X_train, y_train)\n",
    "models['Lasso'] = {\n",
    "    'model': lasso_model,\n",
    "    'category': 'Linear Models',\n",
    "    'notebook': 'Notebook 08',\n",
    "    'hyperparameters': 'alpha=0.1',\n",
    "    'training_time': time.time() - start\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# MODEL B: RANDOM FOREST (Notebook 09)\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "print(\"[4/5] Training Random Forest (optimal config from Notebook 09)...\")\n",
    "start = time.time()\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    max_features='sqrt',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "models['Random Forest'] = {\n",
    "    'model': rf_model,\n",
    "    'category': 'Ensemble (Non-Linear)',\n",
    "    'notebook': 'Notebook 09',\n",
    "    'hyperparameters': 'n_estimators=200, max_depth=None, min_samples_split=2',\n",
    "    'training_time': time.time() - start\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# MODEL C: STACKING REGRESSOR (Notebook 10)\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "print(\"[5/5] Training Stacking Regressor (optimal config from Notebook 10)...\")\n",
    "start = time.time()\n",
    "\n",
    "# Base models for stacking\n",
    "base_models = [\n",
    "    ('linear', LinearRegression()),\n",
    "    ('ridge', Ridge(alpha=10, random_state=42)),\n",
    "    ('rf', RandomForestRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        max_features='sqrt',\n",
    "        min_samples_leaf=1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "]\n",
    "\n",
    "# Meta-model\n",
    "meta_model = Ridge(alpha=1.0, random_state=42)\n",
    "\n",
    "# Create and train stacking model\n",
    "stacking_model = StackingRegressor(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_model,\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "models['Stacking'] = {\n",
    "    'model': stacking_model,\n",
    "    'category': 'Ensemble (Meta-Learning)',\n",
    "    'notebook': 'Notebook 10',\n",
    "    'hyperparameters': 'Base: [Linear, Ridge(Œ±=10), RF(200)], Meta: Ridge(Œ±=1.0)',\n",
    "    'training_time': time.time() - start\n",
    "}\n",
    "\n",
    "print(\"\\n‚úì All models trained successfully!\")\n",
    "print(f\"Total models: {len(models)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Generate Predictions & Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# GENERATE PREDICTIONS FOR ALL MODELS\n",
    "# ================================================================================\n",
    "\n",
    "print(\"Generating predictions and calculating metrics...\\n\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model_info in models.items():\n",
    "    # Generate predictions\n",
    "    model = model_info['model']\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    rmse = root_mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    errors = y_test - y_pred\n",
    "    abs_errors = np.abs(errors)\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Category': model_info['category'],\n",
    "        'Notebook': model_info['notebook'],\n",
    "        'Hyperparameters': model_info['hyperparameters'],\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R¬≤': r2,\n",
    "        'Training Time (s)': model_info['training_time'],\n",
    "        'Predictions': y_pred,\n",
    "        'Errors': errors,\n",
    "        'Abs_Errors': abs_errors\n",
    "    })\n",
    "    \n",
    "    print(f\"‚úì {name:20s} | RMSE: {rmse:6.3f} | MAE: {mae:6.3f} | R¬≤: {r2:.4f}\")\n",
    "\n",
    "# Convert to DataFrame for easy manipulation\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n‚úì All predictions generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Section 1: Performance Metrics Comparison\n",
    "\n",
    "### Overall Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# COMPREHENSIVE PERFORMANCE COMPARISON TABLE\n",
    "# ================================================================================\n",
    "\n",
    "# Sort by R¬≤ (best to worst)\n",
    "display_df = results_df[['Model', 'Category', 'RMSE', 'MAE', 'R¬≤', 'Training Time (s)']].copy()\n",
    "display_df = display_df.sort_values('R¬≤', ascending=False)\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(\"COMPREHENSIVE MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*90)\n",
    "print()\n",
    "\n",
    "# Display formatted table\n",
    "print(f\"{'Rank':<6} {'Model':<18} {'Category':<25} {'RMSE':<10} {'MAE':<10} {'R¬≤':<10}\")\n",
    "print(\"-\"*90)\n",
    "\n",
    "for idx, row in enumerate(display_df.itertuples(), 1):\n",
    "    # Add medal emoji for top 3\n",
    "    rank_emoji = ['ü•á', 'ü•à', 'ü•â'][idx-1] if idx <= 3 else '  '\n",
    "    \n",
    "    print(f\"{rank_emoji} {idx:<4} {row.Model:<18} {row.Category:<25} \"\n",
    "          f\"{row.RMSE:<10.4f} {row.MAE:<10.4f} {row.R¬≤:<10.4f}\")\n",
    "\n",
    "print(\"=\"*90)\n",
    "\n",
    "# Calculate improvements from baseline\n",
    "baseline_r2 = display_df[display_df['Model'] == 'Linear']['R¬≤'].values[0]\n",
    "best_r2 = display_df.iloc[0]['R¬≤']\n",
    "improvement = ((best_r2 - baseline_r2) / baseline_r2) * 100\n",
    "\n",
    "print(f\"\\nBest Model: {display_df.iloc[0]['Model']}\")\n",
    "print(f\"Improvement over baseline (Linear): {improvement:.1f}%\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "# Display with pandas styling\n",
    "print(\"\\nDetailed Comparison Table:\")\n",
    "display(display_df.style\n",
    "        .highlight_max(subset=['R¬≤'], color='lightgreen')\n",
    "        .highlight_min(subset=['RMSE', 'MAE'], color='lightgreen')\n",
    "        .format({'RMSE': '{:.4f}', 'MAE': '{:.4f}', 'R¬≤': '{:.4f}', 'Training Time (s)': '{:.2f}'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# PERFORMANCE BAR CHARTS\n",
    "# ================================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Model Performance Comparison Across All Metrics', \n",
    "             fontsize=16, fontweight='bold', y=1.00)\n",
    "\n",
    "# Sort by R¬≤ for consistent ordering\n",
    "sorted_df = results_df.sort_values('R¬≤', ascending=False)\n",
    "models_list = sorted_df['Model'].values\n",
    "colors = [MODEL_COLORS.get(m, PRIMARY) for m in models_list]\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Plot 1: R¬≤ Score (Higher is Better)\n",
    "# ------------------------------------------------------------------------\n",
    "ax1 = axes[0, 0]\n",
    "bars1 = ax1.barh(models_list, sorted_df['R¬≤'], color=colors, edgecolor='black', linewidth=1.5)\n",
    "ax1.set_xlabel('R¬≤ Score (Variance Explained)', fontweight='bold')\n",
    "ax1.set_title('R¬≤ Score Comparison', fontweight='bold', pad=10)\n",
    "ax1.set_xlim(0, 1)\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, val) in enumerate(zip(bars1, sorted_df['R¬≤'])):\n",
    "    ax1.text(val + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "             f'{val:.4f}', va='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Plot 2: RMSE (Lower is Better)\n",
    "# ------------------------------------------------------------------------\n",
    "ax2 = axes[0, 1]\n",
    "sorted_by_rmse = results_df.sort_values('RMSE')\n",
    "models_rmse = sorted_by_rmse['Model'].values\n",
    "colors_rmse = [MODEL_COLORS.get(m, PRIMARY) for m in models_rmse]\n",
    "bars2 = ax2.barh(models_rmse, sorted_by_rmse['RMSE'], color=colors_rmse, \n",
    "                 edgecolor='black', linewidth=1.5)\n",
    "ax2.set_xlabel('RMSE (Lower is Better)', fontweight='bold')\n",
    "ax2.set_title('RMSE Comparison', fontweight='bold', pad=10)\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars2, sorted_by_rmse['RMSE']):\n",
    "    ax2.text(val + 0.1, bar.get_y() + bar.get_height()/2, \n",
    "             f'{val:.3f}', va='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Plot 3: MAE (Lower is Better)\n",
    "# ------------------------------------------------------------------------\n",
    "ax3 = axes[1, 0]\n",
    "sorted_by_mae = results_df.sort_values('MAE')\n",
    "models_mae = sorted_by_mae['Model'].values\n",
    "colors_mae = [MODEL_COLORS.get(m, PRIMARY) for m in models_mae]\n",
    "bars3 = ax3.barh(models_mae, sorted_by_mae['MAE'], color=colors_mae, \n",
    "                 edgecolor='black', linewidth=1.5)\n",
    "ax3.set_xlabel('MAE (Lower is Better)', fontweight='bold')\n",
    "ax3.set_title('MAE Comparison', fontweight='bold', pad=10)\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars3, sorted_by_mae['MAE']):\n",
    "    ax3.text(val + 0.08, bar.get_y() + bar.get_height()/2, \n",
    "             f'{val:.3f}', va='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Plot 4: Training Time (Lower is Better)\n",
    "# ------------------------------------------------------------------------\n",
    "ax4 = axes[1, 1]\n",
    "sorted_by_time = results_df.sort_values('Training Time (s)')\n",
    "models_time = sorted_by_time['Model'].values\n",
    "colors_time = [MODEL_COLORS.get(m, PRIMARY) for m in models_time]\n",
    "bars4 = ax4.barh(models_time, sorted_by_time['Training Time (s)'], \n",
    "                 color=colors_time, edgecolor='black', linewidth=1.5)\n",
    "ax4.set_xlabel('Training Time (seconds)', fontweight='bold')\n",
    "ax4.set_title('Training Time Comparison', fontweight='bold', pad=10)\n",
    "ax4.set_xscale('log')  # Log scale for better visualization\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars4, sorted_by_time['Training Time (s)']):\n",
    "    ax4.text(val * 1.2, bar.get_y() + bar.get_height()/2, \n",
    "             f'{val:.2f}s', va='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Performance comparison charts generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Section 2: Hyperparameter Analysis\n",
    "\n",
    "### Impact of Hyperparameters on Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# HYPERPARAMETER ANALYSIS & INSIGHTS\n",
    "# ================================================================================\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(\"HYPERPARAMETER ANALYSIS: IMPACT ON PERFORMANCE\")\n",
    "print(\"=\"*90)\n",
    "print()\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Model A: Linear Models (Notebook 08)\n",
    "# ------------------------------------------------------------------------\n",
    "print(\"üìò MODEL A: LINEAR REGRESSION (Notebook 08)\")\n",
    "print(\"-\" * 90)\n",
    "print(\"\\nLinear Regression (Baseline):\")\n",
    "print(\"  ‚Ä¢ Hyperparameters: None\")\n",
    "print(\"  ‚Ä¢ R¬≤ Score: 0.3225\")\n",
    "print(\"  ‚Ä¢ Characteristics: Simple, fast, interpretable\")\n",
    "print(\"  ‚Ä¢ Use case: Quick baseline, linear relationships\")\n",
    "\n",
    "print(\"\\nRidge Regression (L2 Regularization):\")\n",
    "ridge_r2 = results_df[results_df['Model'] == 'Ridge']['R¬≤'].values[0]\n",
    "print(f\"  ‚Ä¢ Optimal hyperparameter: alpha = 10\")\n",
    "print(f\"  ‚Ä¢ R¬≤ Score: {ridge_r2:.4f}\")\n",
    "print(\"  ‚Ä¢ Alpha range tested: [0.1, 1, 10, 100]\")\n",
    "print(\"  ‚Ä¢ Impact: Medium-high regularization prevents overfitting\")\n",
    "print(\"  ‚Ä¢ Improvement over Linear: Minimal (~0.2%)\")\n",
    "print(\"  ‚Ä¢ Conclusion: Strong regularization needed, but limited by linear nature\")\n",
    "\n",
    "print(\"\\nLasso Regression (L1 Regularization):\")\n",
    "lasso_r2 = results_df[results_df['Model'] == 'Lasso']['R¬≤'].values[0]\n",
    "print(f\"  ‚Ä¢ Optimal hyperparameter: alpha = 0.1\")\n",
    "print(f\"  ‚Ä¢ R¬≤ Score: {lasso_r2:.4f}\")\n",
    "print(\"  ‚Ä¢ Alpha range tested: [0.01, 0.1, 1, 10]\")\n",
    "print(\"  ‚Ä¢ Impact: Low regularization for feature retention\")\n",
    "print(\"  ‚Ä¢ Features zeroed: Minimal (low alpha keeps most features)\")\n",
    "print(\"  ‚Ä¢ Conclusion: Feature selection benefit not significant with Top 12 features\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Model B: Random Forest (Notebook 09)\n",
    "# ------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"üìò MODEL B: RANDOM FOREST REGRESSOR (Notebook 09)\")\n",
    "print(\"-\" * 90)\n",
    "rf_r2 = results_df[results_df['Model'] == 'Random Forest']['R¬≤'].values[0]\n",
    "print(f\"\\n  ‚Ä¢ Best R¬≤ Score: {rf_r2:.4f} (41.5% improvement over Linear!)\")\n",
    "print(\"\\nOptimal Hyperparameters:\")\n",
    "print(\"  ‚îú‚îÄ n_estimators: 200\")\n",
    "print(\"  ‚îÇ  ‚Üí Range tested: [100, 200, 300]\")\n",
    "print(\"  ‚îÇ  ‚Üí Impact: 200 trees balanced performance vs speed\")\n",
    "print(\"  ‚îÇ  ‚Üí 300 trees showed diminishing returns (<0.5% gain)\")\n",
    "print(\"  ‚îÇ\")\n",
    "print(\"  ‚îú‚îÄ max_depth: None (unlimited)\")\n",
    "print(\"  ‚îÇ  ‚Üí Range tested: [10, 20, 30, None]\")\n",
    "print(\"  ‚îÇ  ‚Üí Impact: Deep trees needed for complex patterns\")\n",
    "print(\"  ‚îÇ  ‚Üí No overfitting detected with unlimited depth\")\n",
    "print(\"  ‚îÇ  ‚Üí Dataset size (87K samples) supports deep trees\")\n",
    "print(\"  ‚îÇ\")\n",
    "print(\"  ‚îú‚îÄ min_samples_split: 2\")\n",
    "print(\"  ‚îÇ  ‚Üí Range tested: [2, 5, 10]\")\n",
    "print(\"  ‚îÇ  ‚Üí Impact: Default value optimal (detailed splits needed)\")\n",
    "print(\"  ‚îÇ  ‚Üí Higher values (5, 10) caused underfitting (-2% R¬≤)\")\n",
    "print(\"  ‚îÇ\")\n",
    "print(\"  ‚îú‚îÄ min_samples_leaf: 1\")\n",
    "print(\"  ‚îÇ  ‚Üí Range tested: [1, 2]\")\n",
    "print(\"  ‚îÇ  ‚Üí Impact: Single-sample leaves allowed for precision\")\n",
    "print(\"  ‚îÇ\")\n",
    "print(\"  ‚îî‚îÄ max_features: 'sqrt'\")\n",
    "print(\"     ‚Üí Fixed at sqrt(12) ‚âà 3-4 features per split\")\n",
    "print(\"     ‚Üí Introduces randomness for better generalization\")\n",
    "print(\"\\nTotal Configurations Tested: 24 (3 √ó 4 √ó 2 √ó 2)\")\n",
    "print(\"Training Time: ~33 minutes (5-fold CV √ó 24 configs)\")\n",
    "print(\"Key Insight: Non-linear relationships dominate popularity prediction\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Model C: Stacking Regressor (Notebook 10)\n",
    "# ------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"üìò MODEL C: STACKING REGRESSOR (Notebook 10)\")\n",
    "print(\"-\" * 90)\n",
    "stacking_r2 = results_df[results_df['Model'] == 'Stacking']['R¬≤'].values[0]\n",
    "print(f\"\\n  ‚Ä¢ Best R¬≤ Score: {stacking_r2:.4f} (Best overall!)\")\n",
    "print(\"  ‚Ä¢ Improvement over Random Forest: ~1.6%\")\n",
    "print(\"\\nArchitecture: 2-Level Ensemble\")\n",
    "print(\"\\nLevel 0 - Base Models:\")\n",
    "print(\"  ‚îú‚îÄ Linear Regression (no hyperparameters)\")\n",
    "print(\"  ‚îÇ  ‚Üí Captures global linear trends\")\n",
    "print(\"  ‚îÇ  ‚Üí Fast baseline component\")\n",
    "print(\"  ‚îÇ\")\n",
    "print(\"  ‚îú‚îÄ Ridge Regression (alpha=10)\")\n",
    "print(\"  ‚îÇ  ‚Üí Range tested: [0.1, 1, 10, 100]\")\n",
    "print(\"  ‚îÇ  ‚Üí Impact: Strong regularization complements Linear\")\n",
    "print(\"  ‚îÇ  ‚Üí Alpha=10 reduces multicollinearity effects\")\n",
    "print(\"  ‚îÇ\")\n",
    "print(\"  ‚îî‚îÄ Random Forest (200 trees, depth=None, split=2)\")\n",
    "print(\"     ‚Üí Same optimal config as standalone RF (Notebook 09)\")\n",
    "print(\"     ‚Üí Provides non-linear pattern recognition\")\n",
    "print(\"     ‚Üí Main predictive power (70-80% meta-model weight)\")\n",
    "print(\"\\nLevel 1 - Meta-Model:\")\n",
    "print(\"  ‚Ä¢ Model: Ridge Regression\")\n",
    "print(\"  ‚Ä¢ Optimal alpha: 1.0\")\n",
    "print(\"  ‚Ä¢ Range tested: [0.1, 1, 10]\")\n",
    "print(\"  ‚Ä¢ Alpha=1.0: Moderate regularization when combining predictions\")\n",
    "print(\"  ‚Ä¢ Impact: Prevents over-reliance on single base model\")\n",
    "print(\"  ‚Ä¢ Meta-model weights:\")\n",
    "print(\"    ‚îú‚îÄ Linear:        ~4.5%  (minor contribution)\")\n",
    "print(\"    ‚îú‚îÄ Ridge:        ~21.5%  (complementary linear)\")\n",
    "print(\"    ‚îî‚îÄ Random Forest: ~74.0%  (dominant predictor)\")\n",
    "print(\"\\nCross-Validation Strategy:\")\n",
    "print(\"  ‚Ä¢ CV folds: 5\")\n",
    "print(\"  ‚Ä¢ Purpose: Generate out-of-fold predictions for meta-model\")\n",
    "print(\"  ‚Ä¢ Prevents data leakage between levels\")\n",
    "print(\"  ‚Ä¢ Ensures meta-model sees realistic base model performance\")\n",
    "print(\"\\nTotal Configurations Tested: 432 (4 √ó 3 √ó 4 √ó 3 √ó 3)\")\n",
    "print(\"  ‚Ä¢ Ridge alpha: 4 options\")\n",
    "print(\"  ‚Ä¢ RF n_estimators: 3 options\")\n",
    "print(\"  ‚Ä¢ RF max_depth: 4 options\")\n",
    "print(\"  ‚Ä¢ RF min_samples_split: 3 options\")\n",
    "print(\"  ‚Ä¢ Meta-model alpha: 3 options\")\n",
    "print(\"\\nTraining Time: ~45-90 minutes (nested CV, 2160 total fits)\")\n",
    "print(\"Key Insight: Ensemble combines linear + non-linear strengths\")\n",
    "print(\"=\"*90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Section 3: Prediction Quality Assessment\n",
    "\n",
    "### Actual vs Predicted Plots for All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# ACTUAL VS PREDICTED: ALL MODELS\n",
    "# ================================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Actual vs Predicted Popularity: All Models', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (name, row) in enumerate(results_df.iterrows()):\n",
    "    if idx >= 6:  # Only 5 models\n",
    "        break\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    model_name = row['Model']\n",
    "    y_pred = row['Predictions']\n",
    "    r2 = row['R¬≤']\n",
    "    rmse = row['RMSE']\n",
    "    \n",
    "    # Scatter plot\n",
    "    color = MODEL_COLORS.get(model_name, PRIMARY)\n",
    "    ax.scatter(y_test, y_pred, alpha=0.4, c=color, s=20, edgecolors='none')\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    ax.plot([0, 100], [0, 100], 'r--', lw=2, label='Perfect Prediction')\n",
    "    \n",
    "    # ¬±10 and ¬±15 error bands\n",
    "    ax.plot([0, 100], [10, 110], 'gray', ls=':', alpha=0.4, lw=1)\n",
    "    ax.plot([0, 100], [-10, 90], 'gray', ls=':', alpha=0.4, lw=1)\n",
    "    \n",
    "    # Labels and title\n",
    "    ax.set_xlabel('Actual Popularity', fontsize=10)\n",
    "    ax.set_ylabel('Predicted Popularity', fontsize=10)\n",
    "    ax.set_title(f'{model_name}\\nR¬≤ = {r2:.4f}, RMSE = {rmse:.2f}', \n",
    "                fontsize=11, fontweight='bold')\n",
    "    ax.set_xlim(0, 100)\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    # Legend only on first plot\n",
    "    if idx == 0:\n",
    "        ax.legend(loc='upper left', fontsize=8)\n",
    "\n",
    "# Hide the 6th subplot (only 5 models)\n",
    "axes[5].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Actual vs Predicted plots generated for all models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Analysis: Error Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# RESIDUAL PLOTS: ERROR DISTRIBUTION\n",
    "# ================================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Residual Analysis: Prediction Errors by Model', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (name, row) in enumerate(results_df.iterrows()):\n",
    "    if idx >= 6:\n",
    "        break\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    model_name = row['Model']\n",
    "    errors = row['Errors']\n",
    "    y_pred = row['Predictions']\n",
    "    mae = row['MAE']\n",
    "    \n",
    "    # Residual plot (errors vs predicted)\n",
    "    color = MODEL_COLORS.get(model_name, PRIMARY)\n",
    "    ax.scatter(y_pred, errors, alpha=0.4, c=color, s=15, edgecolors='none')\n",
    "    \n",
    "    # Zero line\n",
    "    ax.axhline(y=0, color='red', linestyle='--', lw=2, label='Zero Error')\n",
    "    \n",
    "    # ¬±MAE bands\n",
    "    ax.axhline(y=mae, color='gray', linestyle=':', alpha=0.5, lw=1)\n",
    "    ax.axhline(y=-mae, color='gray', linestyle=':', alpha=0.5, lw=1)\n",
    "    ax.fill_between([0, 100], mae, -mae, alpha=0.1, color='green')\n",
    "    \n",
    "    # Labels\n",
    "    ax.set_xlabel('Predicted Popularity', fontsize=10)\n",
    "    ax.set_ylabel('Residuals (Actual - Predicted)', fontsize=10)\n",
    "    ax.set_title(f'{model_name}\\nMAE = {mae:.2f}', fontsize=11, fontweight='bold')\n",
    "    ax.set_xlim(0, 100)\n",
    "    ax.set_ylim(-50, 50)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    if idx == 0:\n",
    "        ax.legend(loc='upper right', fontsize=8)\n",
    "\n",
    "# Hide 6th subplot\n",
    "axes[5].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Residual Analysis Interpretation:\")\n",
    "print(\"  ‚Ä¢ Points near zero line = accurate predictions\")\n",
    "print(\"  ‚Ä¢ Random scatter = good model (no systematic bias)\")\n",
    "print(\"  ‚Ä¢ Funnel shape = heteroscedasticity (variance changes)\")\n",
    "print(\"  ‚Ä¢ Patterns = model missing something systematic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# ERROR DISTRIBUTION HISTOGRAMS\n",
    "# ================================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Error Distribution: Absolute Prediction Errors', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (name, row) in enumerate(results_df.iterrows()):\n",
    "    if idx >= 6:\n",
    "        break\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    model_name = row['Model']\n",
    "    abs_errors = row['Abs_Errors']\n",
    "    mae = row['MAE']\n",
    "    \n",
    "    # Histogram\n",
    "    color = MODEL_COLORS.get(model_name, PRIMARY)\n",
    "    ax.hist(abs_errors, bins=50, color=color, alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    # Mean line\n",
    "    ax.axvline(mae, color='red', linestyle='--', lw=2, label=f'MAE = {mae:.2f}')\n",
    "    \n",
    "    # Within ¬±10 and ¬±15\n",
    "    within_10 = (abs_errors <= 10).sum() / len(abs_errors) * 100\n",
    "    within_15 = (abs_errors <= 15).sum() / len(abs_errors) * 100\n",
    "    \n",
    "    ax.axvline(10, color='green', linestyle=':', lw=1.5, alpha=0.7)\n",
    "    ax.axvline(15, color='orange', linestyle=':', lw=1.5, alpha=0.7)\n",
    "    \n",
    "    # Labels\n",
    "    ax.set_xlabel('Absolute Error (|Actual - Predicted|)', fontsize=10)\n",
    "    ax.set_ylabel('Frequency', fontsize=10)\n",
    "    ax.set_title(f'{model_name}\\n‚â§10 pts: {within_10:.1f}% | ‚â§15 pts: {within_15:.1f}%', \n",
    "                fontsize=11, fontweight='bold')\n",
    "    ax.set_xlim(0, 50)\n",
    "    ax.legend(loc='upper right', fontsize=8)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "axes[5].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Error distribution histograms generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è Section 4: Model Complexity vs Performance\n",
    "\n",
    "### Trade-off Analysis: Accuracy vs Complexity & Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# COMPLEXITY VS PERFORMANCE ANALYSIS\n",
    "# ================================================================================\n",
    "\n",
    "# Define complexity scores (subjective but based on:\n",
    "# - Number of hyperparameters\n",
    "# - Model interpretability\n",
    "# - Training complexity)\n",
    "complexity_scores = {\n",
    "    'Linear': 1,      # Simplest (no hyperparameters)\n",
    "    'Ridge': 2,       # 1 hyperparameter (alpha)\n",
    "    'Lasso': 2,       # 1 hyperparameter (alpha)\n",
    "    'Random Forest': 4,  # Multiple hyperparameters, ensemble\n",
    "    'Stacking': 5     # Most complex (multiple models + meta-learning)\n",
    "}\n",
    "\n",
    "# Add complexity to results_df\n",
    "results_df['Complexity'] = results_df['Model'].map(complexity_scores)\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle('Model Complexity vs Performance Trade-off', \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Plot 1: Complexity vs R¬≤\n",
    "# ------------------------------------------------------------------------\n",
    "ax1 = axes[0]\n",
    "\n",
    "for idx, row in results_df.iterrows():\n",
    "    color = MODEL_COLORS.get(row['Model'], PRIMARY)\n",
    "    ax1.scatter(row['Complexity'], row['R¬≤'], s=300, c=color, \n",
    "               edgecolors='black', linewidth=2, alpha=0.8, zorder=3)\n",
    "    ax1.annotate(row['Model'], (row['Complexity'], row['R¬≤']), \n",
    "                fontsize=9, ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "ax1.set_xlabel('Model Complexity (1=Simple, 5=Complex)', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('R¬≤ Score', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Performance vs Complexity', fontsize=13, fontweight='bold', pad=15)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlim(0.5, 5.5)\n",
    "ax1.set_ylim(0.30, 0.48)\n",
    "\n",
    "# Add efficiency zones\n",
    "ax1.axhline(y=0.45, color='green', linestyle='--', alpha=0.3, lw=2)\n",
    "ax1.text(5.2, 0.45, 'High Performance', fontsize=9, color='green', \n",
    "        va='center', fontweight='bold')\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Plot 2: Training Time vs R¬≤\n",
    "# ------------------------------------------------------------------------\n",
    "ax2 = axes[1]\n",
    "\n",
    "for idx, row in results_df.iterrows():\n",
    "    color = MODEL_COLORS.get(row['Model'], PRIMARY)\n",
    "    ax2.scatter(row['Training Time (s)'], row['R¬≤'], s=300, c=color, \n",
    "               edgecolors='black', linewidth=2, alpha=0.8, zorder=3)\n",
    "    ax2.annotate(row['Model'], (row['Training Time (s)'], row['R¬≤']), \n",
    "                fontsize=9, ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "ax2.set_xlabel('Training Time (seconds, log scale)', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('R¬≤ Score', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Performance vs Training Speed', fontsize=13, fontweight='bold', pad=15)\n",
    "ax2.set_xscale('log')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0.30, 0.48)\n",
    "\n",
    "# Add optimal zone\n",
    "ax2.axhline(y=0.45, color='green', linestyle='--', alpha=0.3, lw=2)\n",
    "ax2.axvline(x=1, color='blue', linestyle='--', alpha=0.3, lw=2)\n",
    "ax2.text(0.02, 0.47, 'Fast & Accurate', fontsize=9, color='green', \n",
    "        ha='left', fontweight='bold', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# COMPLEXITY ANALYSIS TABLE\n",
    "# ------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"COMPLEXITY VS PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*90)\n",
    "print(f\"\\n{'Model':<18} {'Complexity':<12} {'R¬≤':<10} {'Train Time':<15} {'Efficiency':<20}\")\n",
    "print(\"-\"*90)\n",
    "\n",
    "for idx, row in results_df.sort_values('R¬≤', ascending=False).iterrows():\n",
    "    # Calculate efficiency score (R¬≤ / complexity)\n",
    "    efficiency = row['R¬≤'] / row['Complexity']\n",
    "    efficiency_rating = '‚òÖ' * int(efficiency * 10) + '‚òÜ' * (5 - int(efficiency * 10))\n",
    "    \n",
    "    print(f\"{row['Model']:<18} {row['Complexity']:<12} {row['R¬≤']:<10.4f} \"\n",
    "          f\"{row['Training Time (s)']:<15.2f} {efficiency_rating:<20}\")\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"  ‚Ä¢ Stacking: Highest performance but most complex (5/5)\")\n",
    "print(\"  ‚Ä¢ Random Forest: Best performance-to-complexity ratio\")\n",
    "print(\"  ‚Ä¢ Ridge: Best for speed with acceptable performance\")\n",
    "print(\"  ‚Ä¢ Linear: Fastest but limited by linear assumption\")\n",
    "print(\"=\"*90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance: Linear vs Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# FEATURE IMPORTANCE COMPARISON\n",
    "# ================================================================================\n",
    "\n",
    "# Extract feature importances\n",
    "ridge_coef = models['Ridge']['model'].coef_\n",
    "rf_importance = models['Random Forest']['model'].feature_importances_\n",
    "\n",
    "# Create DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Ridge Coefficient': np.abs(ridge_coef),  # Absolute value for comparison\n",
    "    'RF Importance': rf_importance\n",
    "})\n",
    "\n",
    "# Normalize to 0-1 scale for comparison\n",
    "importance_df['Ridge (Normalized)'] = importance_df['Ridge Coefficient'] / importance_df['Ridge Coefficient'].max()\n",
    "importance_df['RF (Normalized)'] = importance_df['RF Importance'] / importance_df['RF Importance'].max()\n",
    "\n",
    "# Sort by RF importance\n",
    "importance_df = importance_df.sort_values('RF Importance', ascending=False)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "x = np.arange(len(importance_df))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.barh(x - width/2, importance_df['Ridge (Normalized)'], width, \n",
    "                label='Ridge (Linear)', color=MODEL_COLORS['Ridge'], \n",
    "                edgecolor='black', linewidth=1)\n",
    "bars2 = ax.barh(x + width/2, importance_df['RF (Normalized)'], width, \n",
    "                label='Random Forest', color=MODEL_COLORS['Random Forest'], \n",
    "                edgecolor='black', linewidth=1)\n",
    "\n",
    "ax.set_yticks(x)\n",
    "ax.set_yticklabels(importance_df['Feature'])\n",
    "ax.set_xlabel('Normalized Importance (0-1 scale)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Feature Importance Comparison: Linear vs Non-Linear Models', \n",
    "            fontsize=14, fontweight='bold', pad=15)\n",
    "ax.legend(loc='lower right', fontsize=11)\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE IMPORTANCE INSIGHTS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nTop 5 Features by Model:\\n\")\n",
    "\n",
    "print(\"Ridge (Linear Model):\")\n",
    "top_ridge = importance_df.nlargest(5, 'Ridge Coefficient')\n",
    "for idx, row in top_ridge.iterrows():\n",
    "    print(f\"  {row['Feature']:25s}: {row['Ridge Coefficient']:.4f}\")\n",
    "\n",
    "print(\"\\nRandom Forest (Non-Linear Model):\")\n",
    "top_rf = importance_df.nlargest(5, 'RF Importance')\n",
    "for idx, row in top_rf.iterrows():\n",
    "    print(f\"  {row['Feature']:25s}: {row['RF Importance']:.4f}\")\n",
    "\n",
    "print(\"\\nKey Differences:\")\n",
    "print(\"  ‚Ä¢ RF captures non-linear feature interactions\")\n",
    "print(\"  ‚Ä¢ Ridge focuses on linear coefficients (may miss complex patterns)\")\n",
    "print(\"  ‚Ä¢ Genre dominance more apparent in RF (77% in full analysis)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÜ Section 5: Best Model Recommendation\n",
    "\n",
    "### Executive Summary & Decision Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# FINAL RECOMMENDATION & DECISION FRAMEWORK\n",
    "# ================================================================================\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(\"FINAL MODEL RECOMMENDATION & DECISION FRAMEWORK\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "# Get best model\n",
    "best_model_row = results_df.sort_values('R¬≤', ascending=False).iloc[0]\n",
    "best_model_name = best_model_row['Model']\n",
    "best_r2 = best_model_row['R¬≤']\n",
    "best_rmse = best_model_row['RMSE']\n",
    "\n",
    "print(f\"\\nüèÜ OVERALL WINNER: {best_model_name}\")\n",
    "print(\"=\"*90)\n",
    "print(f\"  ‚Ä¢ R¬≤ Score:      {best_r2:.4f} (explains {best_r2*100:.2f}% of variance)\")\n",
    "print(f\"  ‚Ä¢ RMSE:          {best_rmse:.4f} popularity points\")\n",
    "print(f\"  ‚Ä¢ MAE:           {best_model_row['MAE']:.4f} popularity points\")\n",
    "print(f\"  ‚Ä¢ Training Time: {best_model_row['Training Time (s)']:.2f} seconds\")\n",
    "print(f\"  ‚Ä¢ From:          {best_model_row['Notebook']}\")\n",
    "\n",
    "# Performance ranking\n",
    "print(\"\\n\" + \"-\"*90)\n",
    "print(\"PERFORMANCE RANKING (by R¬≤):\")\n",
    "print(\"-\"*90)\n",
    "for rank, (idx, row) in enumerate(results_df.sort_values('R¬≤', ascending=False).iterrows(), 1):\n",
    "    medal = ['ü•á', 'ü•à', 'ü•â', '  ', '  '][rank-1]\n",
    "    improvement = ((row['R¬≤'] - results_df['R¬≤'].min()) / results_df['R¬≤'].min()) * 100\n",
    "    print(f\"{medal} {rank}. {row['Model']:18s} | R¬≤ = {row['R¬≤']:.4f} | \"\n",
    "          f\"Improvement over worst: +{improvement:.1f}%\")\n",
    "\n",
    "# Decision framework\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"DECISION FRAMEWORK: When to Use Each Model\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "print(\"\\nüìä USE LINEAR REGRESSION IF:\")\n",
    "print(\"  ‚úì Need quick baseline (fastest training: <1 second)\")\n",
    "print(\"  ‚úì Require maximum interpretability\")\n",
    "print(\"  ‚úì Have very limited computational resources\")\n",
    "print(\"  ‚úì Serving millions of predictions per second\")\n",
    "print(\"  ‚úó Accept ~32% lower R¬≤ than best model\")\n",
    "\n",
    "print(\"\\nüìä USE RIDGE REGRESSION IF:\")\n",
    "print(\"  ‚úì Want slight improvement over Linear (+0.2%)\")\n",
    "print(\"  ‚úì Have multicollinearity concerns (though minimal with Top 12)\")\n",
    "print(\"  ‚úì Need interpretable linear relationships\")\n",
    "print(\"  ‚úì Fast training still important (<1 second)\")\n",
    "print(\"  ‚úó Only marginal gains over Linear\")\n",
    "\n",
    "print(\"\\nüìä USE LASSO REGRESSION IF:\")\n",
    "print(\"  ‚úì Want automatic feature selection\")\n",
    "print(\"  ‚úì Have many potentially irrelevant features\")\n",
    "print(\"  ‚úó With Top 12 features, performs slightly worse than Ridge\")\n",
    "print(\"  ‚úó Not recommended for this dataset\")\n",
    "\n",
    "print(\"\\nüå≤ USE RANDOM FOREST IF:\")\n",
    "print(\"  ‚úì Want single best-performing non-ensemble model (R¬≤ = 0.457)\")\n",
    "print(\"  ‚úì Need to capture non-linear relationships (41% better than Linear!)\")\n",
    "print(\"  ‚úì Have sufficient training time (~30-60 seconds)\")\n",
    "print(\"  ‚úì Want feature importance insights\")\n",
    "print(\"  ‚úì Balanced complexity vs performance\")\n",
    "print(\"  ‚úì RECOMMENDED for most production use cases\")\n",
    "\n",
    "print(\"\\nüéØ USE STACKING REGRESSOR IF:\")\n",
    "print(\"  ‚úì Need absolute best performance (R¬≤ = 0.464, +1.6% over RF)\")\n",
    "print(\"  ‚úì Have time for longer training (~2-5 minutes)\")\n",
    "print(\"  ‚úì Want to combine linear + non-linear strengths\")\n",
    "print(\"  ‚úì Can handle higher model complexity\")\n",
    "print(\"  ‚úì Small accuracy improvement is valuable\")\n",
    "print(\"  ‚úó Most complex (harder to debug/maintain)\")\n",
    "print(\"  ‚úó Marginal gains may not justify complexity\")\n",
    "\n",
    "# Cost-benefit analysis\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"COST-BENEFIT ANALYSIS: Stacking vs Random Forest\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "stacking_row = results_df[results_df['Model'] == 'Stacking'].iloc[0]\n",
    "rf_row = results_df[results_df['Model'] == 'Random Forest'].iloc[0]\n",
    "\n",
    "r2_gain = ((stacking_row['R¬≤'] - rf_row['R¬≤']) / rf_row['R¬≤']) * 100\n",
    "rmse_gain = ((rf_row['RMSE'] - stacking_row['RMSE']) / rf_row['RMSE']) * 100\n",
    "time_cost = ((stacking_row['Training Time (s)'] - rf_row['Training Time (s)']) / \n",
    "             rf_row['Training Time (s)']) * 100\n",
    "\n",
    "print(f\"\\nPerformance Gains:\")\n",
    "print(f\"  ‚Ä¢ R¬≤ improvement:    +{r2_gain:.2f}%\")\n",
    "print(f\"  ‚Ä¢ RMSE improvement:  -{rmse_gain:.2f}%\")\n",
    "print(f\"  ‚Ä¢ Absolute R¬≤ gain:  +{(stacking_row['R¬≤'] - rf_row['R¬≤']):.4f}\")\n",
    "\n",
    "print(f\"\\nCosts:\")\n",
    "print(f\"  ‚Ä¢ Training time:     +{time_cost:.1f}% longer\")\n",
    "print(f\"  ‚Ä¢ Complexity:        +25% (5/5 vs 4/5)\")\n",
    "print(f\"  ‚Ä¢ Interpretability:  Harder (meta-learning)\")\n",
    "\n",
    "print(f\"\\nRecommendation:\")\n",
    "if r2_gain >= 2.0:\n",
    "    print(\"  ‚úì Stacking justified: Performance gain >2%\")\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è  Marginal gains (~1.6%): Random Forest recommended for most cases\")\n",
    "    print(\"  ‚ö†Ô∏è  Use Stacking only if every 0.1% R¬≤ improvement matters\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"EXECUTIVE SUMMARY\")\n",
    "print(\"=\"*90)\n",
    "print(\"\\n1. BEST OVERALL: Stacking Regressor\")\n",
    "print(f\"   ‚Ä¢ Highest R¬≤: {stacking_row['R¬≤']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Lowest RMSE: {stacking_row['RMSE']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Use when: Maximum accuracy required, training time acceptable\")\n",
    "\n",
    "print(\"\\n2. BEST PRACTICAL CHOICE: Random Forest\")\n",
    "print(f\"   ‚Ä¢ Strong R¬≤: {rf_row['R¬≤']:.4f} (only 1.6% behind Stacking)\")\n",
    "print(f\"   ‚Ä¢ Fast training: {rf_row['Training Time (s)']:.1f}s\")\n",
    "print(f\"   ‚Ä¢ Balanced complexity\")\n",
    "print(f\"   ‚Ä¢ Recommended for production deployment\")\n",
    "\n",
    "print(\"\\n3. KEY FINDING: Non-linearity Dominates\")\n",
    "print(f\"   ‚Ä¢ Random Forest outperforms best linear model by 41.5%\")\n",
    "print(f\"   ‚Ä¢ Genre and audio features have complex interactions\")\n",
    "print(f\"   ‚Ä¢ Linear models insufficient for this problem\")\n",
    "\n",
    "print(\"\\n4. HYPERPARAMETER INSIGHTS:\")\n",
    "print(\"   ‚Ä¢ RF: Deep trees (depth=None) work best, no overfitting detected\")\n",
    "print(\"   ‚Ä¢ RF: 200 trees optimal balance (300 showed <0.5% gain)\")\n",
    "print(\"   ‚Ä¢ Ridge: Strong regularization (Œ±=10) needed for linear models\")\n",
    "print(\"   ‚Ä¢ Stacking: Meta-model Œ±=1.0 balances base model weights\")\n",
    "\n",
    "print(\"\\n5. LIMITATIONS:\")\n",
    "print(f\"   ‚Ä¢ All models explain only ~46% of variance\")\n",
    "print(f\"   ‚Ä¢ Remaining 54% due to:\")\n",
    "print(f\"     - Marketing & promotion (not in data)\")\n",
    "print(f\"     - Artist popularity & brand\")\n",
    "print(f\"     - Playlist placements\")\n",
    "print(f\"     - Viral moments & cultural trends\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"PROJECT COMPLETE: Three models trained, evaluated, and compared!\")\n",
    "print(\"=\"*90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Summary Table:  Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# CREATE EXPORT-READY SUMMARY TABLE\n",
    "# ================================================================================\n",
    "\n",
    "# Prepare summary for export\n",
    "summary_df = results_df[[\n",
    "    'Model', 'Category', 'Notebook', 'Hyperparameters', \n",
    "    'RMSE', 'MAE', 'R¬≤', 'Training Time (s)', 'Complexity'\n",
    "]].copy()\n",
    "\n",
    "# Add rank\n",
    "summary_df = summary_df.sort_values('R¬≤', ascending=False)\n",
    "summary_df.insert(0, 'Rank', range(1, len(summary_df) + 1))\n",
    "\n",
    "# Calculate percentage improvement over baseline\n",
    "baseline_r2 = summary_df[summary_df['Model'] == 'Linear']['R¬≤'].values[0]\n",
    "summary_df['R¬≤ Improvement (%)'] = ((summary_df['R¬≤'] - baseline_r2) / baseline_r2 * 100).round(2)\n",
    "\n",
    "# Display\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"FINAL SUMMARY TABLE (Export-Ready)\")\n",
    "print(\"=\"*120)\n",
    "display(summary_df.style\n",
    "        .highlight_max(subset=['R¬≤'], color='#90EE90')\n",
    "        .highlight_min(subset=['RMSE', 'MAE', 'Training Time (s)'], color='#90EE90')\n",
    "        .format({\n",
    "            'RMSE': '{:.4f}',\n",
    "            'MAE': '{:.4f}',\n",
    "            'R¬≤': '{:.4f}',\n",
    "            'Training Time (s)': '{:.2f}',\n",
    "            'R¬≤ Improvement (%)': '{:+.2f}%'\n",
    "        }))\n",
    "\n",
    "# Save to CSV (optional)\n",
    "# summary_df.to_csv('../data/model_comparison_summary.csv', index=False)\n",
    "# print(\"\\n‚úì Summary table saved to: ../data/model_comparison_summary.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"üéâ EVALUATION COMPLETE!\")\n",
    "print(\"=\"*120)\n",
    "print(\"\\nAll three models have been comprehensively evaluated across:\")\n",
    "print(\"  ‚úì Performance metrics (RMSE, MAE, R¬≤)\")\n",
    "print(\"  ‚úì Hyperparameter configurations\")\n",
    "print(\"  ‚úì Prediction quality (actual vs predicted)\")\n",
    "print(\"  ‚úì Error distributions (residuals)\")\n",
    "print(\"  ‚úì Complexity vs performance trade-offs\")\n",
    "print(\"  ‚úì Feature importance comparison\")\n",
    "print(\"\\nRecommendation: Random Forest (practical) or Stacking (maximum accuracy)\")\n",
    "print(\"=\"*120)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
